# What is Continual Learning?

## Introduction 
- Deep Learning hold *state-of-the-art* performances in many tasks due to supervised training on **huge** and **fixed** datasets.

- Main problem being solved by Deep Learning: **Curse of Dimensionality**.

- How can we improve AI **efficiency**, **scalability** and **adaptability** to make it sustainable in the long term?

## Continual Learning Goal

- Higher and **realistic time-scale** where data (and tasks) become available only during a time.
- **No access** to previously encoundered data.
- **Constant** computational and memory resources (Efficiency).
- **Incremental development** of ever more complex knowledge and skills (Scalability).
- **Efficiency + Scalability = Sustainability**.


## Catastropic Forgetting 

*Catastrophic interference*, also known as ***catastropic forgetting***, is the tendency of neural network to completely and abruptly ***forget*** previously learned information upon learning new information. Mostly due to Gradient Descent. 

## Stress Point

Continual learning aim is not for incremental improvement of *SOTA*, but rather paradigm-changing approaches to machine learning that enable systems to **continously improve based on experience**.

In continual learning we can only access the **current data** from sequence series of experience $S = e_1, ..., e_n$:

```mermaid
flowchart LR
    A[e<sub>1</sub>] --> B[e<sub>2</sub>] --> C[e<sub>3</sub>] --> D[...] --> E[e<sub>n</sub>]
    A ~~~ |Experiences| E
```

## Continual Learning Formulation

A continual learning algorithm $A^{CL}$ is a function with the following signature:

$$A^{CL}: \langle f^{CL}_{i-1}, D^i_{train}, M_{i-1}, t_{i} \rangle \rightarrow \langle f^{CL}, M_{i} \rangle,$$

where 



# Relationship with Other Learning Paradigms

# Brief History of Continual Learning