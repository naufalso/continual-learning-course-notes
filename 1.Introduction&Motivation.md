# What is Continual Learning?

## Introduction

- Deep Learning hold *state-of-the-art* performances in many tasks due to supervised training on **huge** and **fixed** datasets.

- Main problem being solved by Deep Learning: **Curse of Dimensionality**.

- How can we improve AI **efficiency**, **scalability** and **adaptability** to make it sustainable in the long term?

## Continual Learning Goal

- Higher and **realistic time-scale** where data (and tasks) become available only during a time.
- **No access** to previously encoundered data.
- **Constant** computational and memory resources (Efficiency).
- **Incremental development** of ever more complex knowledge and skills (Scalability).
- **Efficiency + Scalability = Sustainability**.

## Catastropic Forgetting

*Catastrophic interference*, also known as ***catastropic forgetting***, is the tendency of neural network to completely and abruptly ***forget*** previously learned information upon learning new information. Mostly due to Gradient Descent.

## Stress Point

Continual learning aim is not for incremental improvement of *SOTA*, but rather paradigm-changing approaches to machine learning that enable systems to **continously improve based on experience**.

In continual learning we can only access the **current data** from sequence series of experience $S = e_1, ..., e_n$:

```mermaid
flowchart LR
    A[e<sub>1</sub>] --> B[e<sub>2</sub>] --> C[e<sub>3</sub>] --> D[...] --> E[e<sub>n</sub>]
    A ~~~ |Experiences| E
```

## Continual Learning Formulation

### Signature of the Continual Learning Algorithm

A continual learning algorithm $A^{CL}$ is defined as a function with the following signature:

$$A^{CL}: \langle f^{CL}_{i-1}, D^i_{train}, M_{i-1}, t_{i} \rangle \rightarrow \langle f^{CL}, M_{i} \rangle,$$

- $f^{CL}$: The model learned after training on experience $e_i$.
- $D^i$: A batch of samples from experience $e_i$, consisting of a tuple $\left \langle x^i, y^i \right \rangle$.
- $M_{i}$: A buffer of past knowledge, such as previous samples or activations, stored from previous experiences, usually of a fixed size.
- $t_i$: A task label which may be used to identify the correct data distribution.

### Objective of the Continual Learning Algorithm

The objective of the continual learning algorithm ($CL$) is to minimize the loss $L_s$ over the entire stream of data $S$:

$$L_S(f_{n}^{CL}, n) = \frac{1}{\sum^n_{i=1} \left | D^i_{test} \right |} \sum_{i=1}^{n} L_{exp}(f_n^{CL}, D^i_{test}),$$

- $L_s$: The loss function over the entire stream of data $S$.
- $f_{n}^{CL}$: The model after training on the $n$-th experience.
- $D^i_{test}$: A batch of test samples from experience $e_i$.
- $L_{exp}(f_n^{CL}, D^i_{test})$: The loss computed on a single sample $\left \langle x, y \right \rangle$, such as cross-entropy in classification problems.

In summary, the continual learning algorithm is designed to update the model ($f^{CL}$), based on new experiences ($D^i_{train}$), store relevant information in a knowledge buffer ($M_{i}$), and minimize the loss over the entire stream of data ($L_S$) to adapt to changing tasks or data distributions over time.+

### Requirements of Coninual Learning Algorithm

1. **Replay-Free Continual Learning**:
The goal is to enable continual learning without relying on the strategy of replaying or revisiting past experiences (previous data). In other words, the algorithm should be capable of learning from new data without needing to explicitly go back and train on all the previously seen data. Because constantly replaying all past data may not be feasible in real-world scenarios where data is abundant or expensive to store and process.

2. **Memory and Computationally Bounded**:
The algorithm should operate within constraints, both in terms of memory and computation. It should be designed to work with limited resources and not excessively consume memory or computational power. Because in the real-world applications often have resource constraints, and a CL algorithm that is memory and computationally bounded is more practical and scalable.

3. **Task-Free Continual Learning**:
The algorithm should be able to learn from new experiences without explicitly knowing the task labels. In other words, it should be able to adapt to new types of data or tasks without task-specific information. Because the nature of tasks might change overtime or label are not always available.

4. **Online Continual Learning**:
The learning process should be continuous and incremental, adapting to new data as it arrives. The algorithm should be capable of learning and updating the model in an online fashion without requiring batch processing of all data. Because data come continously and the model need to adapt quickly without waiting for all data being collected before updating.

# Relationship with Other Learning Paradigms

# Brief History of Continual Learning
